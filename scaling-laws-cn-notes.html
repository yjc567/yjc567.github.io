<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" href="https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="https://raw.githubusercontent.com/yujiachen-y/yjc567.github.io/main/pictures/icon.JPG">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>Scaling Laws for Neural Language Models&nbsp;|&nbsp;Jiachen’s Blog</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="Scaling Laws for Neural Language Models">
  
    <meta name="description" content="个人对 Scaling Laws 的读书小记">
    <meta property="og:description" content="个人对 Scaling Laws 的读书小记">
  
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="https://raw.githubusercontent.com/yujiachen-y/yjc567.github.io/main/pictures/icon.JPG"></span>&nbsp;
      
      <span>主页</span>
    </div>
  </a>
  <span class="Navbar__Delim">&centerdot;</span>
  <a href="tag/%E6%9D%82%E6%96%87.html">
    <div class="Navbar__Btn">
      <span>杂文</span>
    </div>
  </a>
  <span class="Navbar__Delim">&centerdot;</span>
  <a href="tag/%E6%8A%80%E6%9C%AF.html">
    <div class="Navbar__Btn">
      <span>技术</span>
    </div>
  </a>
  <span class="Navbar__Delim">&centerdot;</span>
  <a href="tag/%E9%98%85%E8%AF%BB.html">
    <div class="Navbar__Btn">
      <span>阅读</span>
    </div>
  </a>
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="other.html">
        <div class="Navbar__Btn">
          
          <span>其他</span>
        </div>
      </a>
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
          <span>关于</span>
        </div>
      </a>
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
    <h1 class="Header__Title">Scaling Laws for Neural Language Models</h1>
    
      <div class="DateTagBar">
        
          <span class="DateTagBar__Item DateTagBar__Date">Posted on 2024年11月17日周日</span>
        
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--gray">
            <a href="tag/技术.html">技术</a>
          </span>
        
      </div>
    
  </header>
  <article id="https://www.notion.so/14117ada90718084b14bfa39d96058c1" class="PageRoot"><div id="https://www.notion.so/14117ada907180a5be43f11d61346a0b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">原文： </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a></span></span></p></div><h1 id="https://www.notion.so/14117ada9071808698d1e4c2b9927623" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/14117ada9071808698d1e4c2b9927623"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">1. Performance depends strongly on scale, weakly on model shape</strong></span></span></h1><div id="https://www.notion.so/14117ada90718023a8eefb5741d13fe9" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">模型的性能表现主要和下面 3 个参数有关，而和其他的训练超参数关联不大。</span></span></p></div><ol class="NumberedListWrapper"><li id="https://www.notion.so/14117ada9071807abf08f05935d39315" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">The number of model parameters </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">N</em></span><span class="SemanticString"> (excluding embeddings</span></span></li><li id="https://www.notion.so/14117ada90718006b3c9ecaf6bffbcd7" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">The size of the dataset </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">D</em></span></span></li><li id="https://www.notion.so/14117ada907180339720da1103126a33" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">The amount of compute </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">C</em></span></span></li></ol><div id="https://www.notion.so/14117ada9071809b99cfe55073034c14" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/14117ada9071802f9f32f5b06c285a93" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/14117ada9071802f9f32f5b06c285a93"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">2. Smooth power laws</strong></span></span></h1><div id="https://www.notion.so/14117ada907180d8a74cefc3f9ecfbdc" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">模型的性能和 </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">N D C</em></span><span class="SemanticString"> 3 个要素是幂次关系，当其中 2 个要素不是限制的时候，把剩下的 1 个要素提升 1 个数量级，可以得到模型 loss 的线性下降。</span></span></p></div><div id="https://www.notion.so/14117ada9071805c8112f09be352ac43" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fde5224d2-fa71-47d9-8e16-118ba97a21c2%2F29a67e5a-7193-4309-80a3-b1b915a2cd89%2FDataset_Size.png?width=2304&amp;table=block&amp;id=14117ada-9071-805c-8112-f09be352ac43"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fde5224d2-fa71-47d9-8e16-118ba97a21c2%2F29a67e5a-7193-4309-80a3-b1b915a2cd89%2FDataset_Size.png?width=2304&amp;table=block&amp;id=14117ada-9071-805c-8112-f09be352ac43" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h1 id="https://www.notion.so/14117ada90718009bc57d7b7a627f2f5" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/14117ada90718009bc57d7b7a627f2f5"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">3. Universality of overfitting</strong></span></span></h1><div id="https://www.notion.so/14117ada9071806294a1dab1150aaec8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">如果 </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">N</em></span><span class="SemanticString"> 和 </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">D</em></span><span class="SemanticString"> 只有一个增加，那么模型性能的提升会边际递减，其中的比例关系是 (</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">N</em></span><span class="SemanticString"> ^ 0.74) / </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">D</em></span><span class="SemanticString">，也就是说当模型大小 </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">N</em></span><span class="SemanticString"> 增加 8 倍时，数据量 </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">D</em></span><span class="SemanticString"> 需要且只需要增加 5 倍。</span></span></p></div><h1 id="https://www.notion.so/14117ada9071806bb528ce8f20ab267d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/14117ada9071806bb528ce8f20ab267d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">4. Universality of training</strong></span></span></h1><div id="https://www.notion.so/14117ada9071807bb56bcd12a041d103" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">训练曲线（训练时间和 test loss 的关系）与模型大小无关，且符合幂次关系。</span></span></p></div><div id="https://www.notion.so/14117ada907180658ceee5960a70b32e" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fde5224d2-fa71-47d9-8e16-118ba97a21c2%2Fa5893343-feca-4be2-9450-0f49bf9b7499%2FFigure_2_We_show_a_series_of_language_model_training_runs_with_models_ranging_in_size_from_10_to_10.png?width=1002.6484375&amp;table=block&amp;id=14117ada-9071-8065-8cee-e5960a70b32e"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fde5224d2-fa71-47d9-8e16-118ba97a21c2%2Fa5893343-feca-4be2-9450-0f49bf9b7499%2FFigure_2_We_show_a_series_of_language_model_training_runs_with_models_ranging_in_size_from_10_to_10.png?width=1002.6484375&amp;table=block&amp;id=14117ada-9071-8065-8cee-e5960a70b32e" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h1 id="https://www.notion.so/14117ada9071803fbdd5ec11eb6d182f" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/14117ada9071803fbdd5ec11eb6d182f"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">5. Transfer improves with test performance</strong></span></span></h1><div id="https://www.notion.so/14117ada907180419e23d37471956340" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">当模型面对和训练数据不同分布的测试集时，其表现下降值是一个常量——也就是说，只要模型在训练集上的性能有提升，那么在不同分布的测试集上的表现也会随之提升。</span></span></p></div><div id="https://www.notion.so/14117ada9071806d94f3da40e3d4c1c2" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fde5224d2-fa71-47d9-8e16-118ba97a21c2%2F0ea758c8-4bed-4f75-8f32-5bce232e1bef%2FWikipedia_during_training.png?width=1002.6484375&amp;table=block&amp;id=14117ada-9071-806d-94f3-da40e3d4c1c2"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fde5224d2-fa71-47d9-8e16-118ba97a21c2%2F0ea758c8-4bed-4f75-8f32-5bce232e1bef%2FWikipedia_during_training.png?width=1002.6484375&amp;table=block&amp;id=14117ada-9071-806d-94f3-da40e3d4c1c2" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h1 id="https://www.notion.so/14117ada907180a7a883d8f01c8dc80b" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/14117ada907180a7a883d8f01c8dc80b"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">6. Sample efficiency</strong></span></span></h1><div id="https://www.notion.so/14117ada907180639b32c2f6e4a55e0a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">大模型比小模型更高效，其需要更少的训练次数和数据量就可以达到和小模型相同的表现。</span></span></p></div><div id="https://www.notion.so/14117ada907180978babfbc7089bb832" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fde5224d2-fa71-47d9-8e16-118ba97a21c2%2F54fc6d67-221c-4fe4-bca2-d135cf90c8bd%2FLoss_vs_Model_and_Dataset_Size.png?width=2098&amp;table=block&amp;id=14117ada-9071-8097-8bab-fbc7089bb832"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fde5224d2-fa71-47d9-8e16-118ba97a21c2%2F54fc6d67-221c-4fe4-bca2-d135cf90c8bd%2FLoss_vs_Model_and_Dataset_Size.png?width=2098&amp;table=block&amp;id=14117ada-9071-8097-8bab-fbc7089bb832" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h1 id="https://www.notion.so/14117ada90718027bddde50ddad61599" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/14117ada90718027bddde50ddad61599"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">7. Convergence is inefficient</strong></span></span></h1><div id="https://www.notion.so/14117ada9071803bad4adfc4fd0afa02" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">当计算资源 </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">C</em></span><span class="SemanticString"> 固定，而模型大小 </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">N</em></span><span class="SemanticString"> 和可用数据 </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">D</em></span><span class="SemanticString"> 没有限制时，较优的训练方法是尽可能去训练一个大模型，这优于把计算资源投入在小模型上（使其尽可能收敛）。在这个策略下，有 </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">D</em></span><span class="SemanticString"> ~ </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">C</em></span><span class="SemanticString"> ^ 0.27 这样一个关系。</span></span></p></div><div id="https://www.notion.so/14117ada907180e3a28deb01d79536fd" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fde5224d2-fa71-47d9-8e16-118ba97a21c2%2F9089a283-cc35-431b-81ce-5df1447a1258%2F10x_Serial_Steps.png?width=1002.65625&amp;table=block&amp;id=14117ada-9071-80e3-a28d-eb01d79536fd"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fde5224d2-fa71-47d9-8e16-118ba97a21c2%2F9089a283-cc35-431b-81ce-5df1447a1258%2F10x_Serial_Steps.png?width=1002.65625&amp;table=block&amp;id=14117ada-9071-80e3-a28d-eb01d79536fd" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h1 id="https://www.notion.so/14117ada907180c59316fd8267fbbe31" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/14117ada907180c59316fd8267fbbe31"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">8. Optimal batch size</strong></span></span></h1><div id="https://www.notion.so/14117ada907180d88222df7d5125e516" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">理想的训练 batch size 是损失函数的幂次关系。</span></span></p></div><div id="https://www.notion.so/14117ada90718063a116ca2332c0eb53" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div></article>
  <footer class="Footer">
</footer>
</body>

</html>